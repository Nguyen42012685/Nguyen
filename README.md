Ce projet a été l'occasion d'explorer un domaine fascinant de l'intelligence artifi- cielle (IA): la compréhension et l'analyse des mécanismes de sécurité qui régissent ces systèmes. En particulier, nous nous sommes intéressés à la notion de jailbreak d'une IA, une pratique consistant à contourner les restrictions mises en place pour limiter son accès ou ses capacités. Cette démarche a été motivée par un désir de mieux com- prendre comment ces sécurités sont conçues et, plus encore, comment elles peuvent être contournées. Le processus de jailbreak d'une IA est à la fois technique et conceptuel. Il repose sur l'identification des mécanismes de sécurité intégrés dans le système, puis sur la conception de moyens pour les désactiver ou les contourner. Ce projet nous a permis de plonger dans des aspects techniques complexes tels que la détection de barrières de sécurité, l'analyse de comportements anormaux et l'exploitation des vulnérabilités de Ι'ΙΑ. Au-delà de l'aspect purement technique, cette analyse soulève également des in- terrogations éthiques importantes. En effet, bien que comprendre les mécanismes de sécurité des IA soit crucial pour renforcer leur fiabilité et leur robustesse, il est éga- lement essentiel de se questionner sur les dangers potentiels d'une telle maîtrise. Le but de ce projet n'est donc pas seulement de comprendre comment fonctionnent ces systèmes, mais aussi de réfléchir aux enjeux liés à leur sécurité et aux risques associés à des pratiques comme le jailbreak dans des contextes réels.
